# Docker Compose f√ºr KIFF AI Deployment

services:
  # Ollama (LLM Server) - For local dev: use native Windows Ollama instead (better RAM usage)
  # Uncomment for production deployment:
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: kiff-ollama
  #   ports:
  #     - "11434:11434"
  #   environment:
  #     - OLLAMA_KEEP_ALIVE=5m
  #   volumes:
  #     - ollama_data:/root/.ollama
  #     - D:/AI-Models/llama:/models:ro
  #   restart: unless-stopped

  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: kiff-qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped

  # Backend API
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: kiff-backend
    ports:
      - "8000:8000"
    environment:
      - QDRANT_URL=http://qdrant:6333
      - LLM_SERVER_URL=${LLM_SERVER_URL:-http://host.docker.internal:11434}
    depends_on:
      - qdrant
    volumes:
      - ./backend/config:/app/config
      - ./backend/cache:/app/cache
      - ./documents:/app/documents
    restart: unless-stopped

  # Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: kiff-frontend
    ports:
      - "5173:80"
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  qdrant_data:
  # ollama_data:  # Uncomment if using Docker Ollama
  ollama_data:
